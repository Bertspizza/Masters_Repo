{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section for running Average Mutual Information Analysis (AMI) and Analysis of False Nearest Neighbours (FNN)\n",
    "### To execute the analyses, the processed OpenPose data is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports and functions for AMI\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils_dir.ami_utils import ami, plot_ami_multiple\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_ami(dir, columns, conditions, min_lag, max_lag, exclude = [], all=False):\n",
    "    \"\"\"\n",
    "    Computes Average Mutual Information (AMI) values for specified time series columns\n",
    "    across conditions.\n",
    "\n",
    "    Parameters:\n",
    "        dir : str\n",
    "            The directory path where CSV files containing time series data are stored.  \n",
    "        columns : list of str\n",
    "            The names of the columns (time series) for which AMI should be calculated.   \n",
    "        conditions : list of str\n",
    "            List of condition labels (e.g., ['trial0', 'trail1', 'trial2']) \n",
    "        min_lag : int\n",
    "            Minimum time lag to consider when calculating AMI.  \n",
    "        max_lag : int\n",
    "            Maximum time lag to consider when calculating AMI. Will be adjusted downward\n",
    "            if any file contains fewer data points than this value.    \n",
    "        exclude : list of str, optional\n",
    "            List of participant IDs to exclude based on the filename prefix.\n",
    "        all: bool\n",
    "            If true, computes ami across all conditions instead of within each condition.\n",
    "\n",
    "        Returns:\n",
    "            ami_dict : dict\n",
    "                Nested dictionary structured as {condition: {column: [ami_values]}}, where\n",
    "                ami_values are lists of AMI values for each file and column within that condition.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all:\n",
    "        ami_dict = {condition: {column: [] for column in columns} for condition in conditions}\n",
    "    else:\n",
    "        ami_dict = {'all': {column: [] for column in columns}}\n",
    "    \n",
    "    all_files = [f for f in os.listdir(dir) if f.endswith('.csv')]\n",
    "    files = []\n",
    "    for f in all_files:\n",
    "        id = f.split('_')[0]\n",
    "        if id not in exclude:\n",
    "            files.append(f)\n",
    "        else:\n",
    "            print(f\"Excluded file: {f}\")\n",
    "\n",
    "    # Find the shortest length of all time series\n",
    "    min_length = float('inf')\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dir, file), sep=',')\n",
    "        try:\n",
    "            length = len(df[columns].dropna())\n",
    "            if length < min_length:\n",
    "                min_length = length\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing columns in {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Adjust max_lag if the value of minlength is smaller than max_lag.\n",
    "    if min_length < max_lag:\n",
    "        print(f\"Adjusted max_lag from {max_lag} to {min_length} due to short file.\")\n",
    "        max_lag = min_length\n",
    "\n",
    "    # Main loop\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dir, file), sep=',')\n",
    "        file = file.replace('-', '_')\n",
    "        condition = file.split('_')[4]\n",
    "\n",
    "        if condition not in conditions:\n",
    "            continue\n",
    "\n",
    "        filtered_df = df[columns].iloc[1:] # Remove the first row with NaN values\n",
    "        filtered_df_interpolated = filtered_df.interpolate()\n",
    "\n",
    "        for col in filtered_df_interpolated:\n",
    "            if col in columns:\n",
    "                time_series = filtered_df_interpolated[col]\n",
    "                ami_values = ami(time_series, min_lag=min_lag, max_lag=max_lag)\n",
    "                if not all:\n",
    "                    ami_dict[condition][col].append(ami_values.tolist())\n",
    "                else:\n",
    "                    ami_dict['all'][col].append(ami_values.tolist())\n",
    "            else: \n",
    "                print(f'Column {col} not found.')\n",
    "                continue\n",
    "\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return ami_dict\n",
    "\n",
    "\n",
    "def obtain_avg_ami (ami_results, conditions, columns):\n",
    "    \"\"\"\n",
    "    Get the average ami values within or across conditions\n",
    "\n",
    "    Parameters:\n",
    "        ami_results: dict\n",
    "            Dictionary containing AMI results obtained from run_ami\n",
    "        conditions: list \n",
    "            Conditions to get average AMI for.\n",
    "            If all=True in run_ami, this should = ['all']\n",
    "        columns: list\n",
    "            List of columns to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: dict \n",
    "            A dictionary with average AMI values. \n",
    "    \"\"\"\n",
    "\n",
    "    avg_dict = {condition: {column: [] for column in columns} for condition in conditions}\n",
    "\n",
    "    for condition in conditions:\n",
    "        for column in columns:\n",
    "            series_list = ami_results[condition][column]\n",
    "            \n",
    "            if series_list:\n",
    "                # Find the shortest length among the AMI vectors\n",
    "                min_len = min(len(s) for s in series_list)\n",
    "                # Truncate each vector to the same length\n",
    "                filtered = [s[:min_len] for s in series_list]\n",
    "                \n",
    "                data = np.array(filtered)\n",
    "                averages = np.mean(data, axis=0)\n",
    "                avg_dict[condition][column].append(averages.tolist())\n",
    "            else:\n",
    "                print(f\"No data for condition '{condition}' and column '{column}'.\")\n",
    "\n",
    "    return avg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AMI on the time series\n",
    "\n",
    "directory = \"/.../Processed_Timeseries\" # Set directory to processed data.\n",
    "conditions = ['trial0', 'trial1', 'trial2'] # Set conditions\n",
    "columns = ['headRel_ed_vel', 'body_ed_vel'] # Set columns to evaluate\n",
    "\n",
    "ami_results = {} # Directory to store results \n",
    "\n",
    "ami_results = run_ami(dir=directory, columns=columns, conditions=conditions, min_lag=0, max_lag=100, all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get averages.\n",
    "\n",
    "ami_averages = {} # Directory to store average results\n",
    "\n",
    "ami_averages = obtain_avg_ami (ami_results=ami_results, conditions=['all'], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "\n",
    "conditions=['all'] # If all=True in Run AMI, set to ['all'], otherwise use the conditions original list. \n",
    "\n",
    "for condition in conditions:\n",
    "    for column in columns:\n",
    "        ami_data_for_condition_column = {}\n",
    "\n",
    "        #task_averages = ami_averages.get(condition, {})\n",
    "        task_data = ami_averages.get(condition, {}).get(column)\n",
    "\n",
    "        if task_data:\n",
    "            ami_data_for_condition_column[condition] = task_data\n",
    "\n",
    "        if not ami_data_for_condition_column:\n",
    "            print(f\"No data available for condition '{condition}' and column '{column}'.\")\n",
    "            continue\n",
    "\n",
    "        plot_ami_multiple(\n",
    "            ami_data_for_condition_column,\n",
    "            condition=condition,\n",
    "            column=column,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Nearest Neighbours (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports and functions for FNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils_dir.fnn_utils import fnn, plot_fnn\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the FNN Analysis\n",
    "\n",
    "directory = \"/.../Processed_Timeseries\"  # Set directory to processed data.\n",
    "plot_image = False # Plots image for each FNN. \n",
    "\n",
    "fnn_all = [] # Store all the FNN results.\n",
    "\n",
    "file_names = os.listdir(directory)\n",
    "\n",
    "for file_name in file_names:\n",
    "\n",
    "    if not file_name.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    print(f'Loading file: {file_name}')\n",
    "\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    data = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "    continuous_data = data['body_ed_vel'].dropna() # Select continuous time series and remove NaNs due to widnows velocity calculatiuons. \n",
    "\n",
    "    # Compute FNN (up to 10 dimensions)\n",
    "    fnn_ds, fnn_percent = fnn(continuous_data, tlag=5, min_dimension=1, max_dimension=10)\n",
    "\n",
    "\n",
    "    # Collect results\n",
    "    fnn_all.append(fnn_percent)  # First 10 FNN percentages\n",
    "\n",
    "    # Optional: plot\n",
    "    if plot_image:\n",
    "        plot_fnn(fnn_ds, fnn_percent)\n",
    "\n",
    "    print('FNN computed successfully!')\n",
    "\n",
    "\n",
    "# Plot average FNN across all files\n",
    "if fnn_all:\n",
    "    clear_output(wait=True)\n",
    "    fnn_all = np.array(fnn_all)\n",
    "    avg_fnn = np.mean(fnn_all, axis=0)\n",
    "    print(\"Average FNN (first 10 dimensions):\")\n",
    "    plot_fnn(dimensions=fnn_ds, fnn_percentages=avg_fnn, header='Upper Body Movement')\n",
    "    print(avg_fnn)\n",
    "else:\n",
    "    print(\"No FNN data computed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CouplesEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
